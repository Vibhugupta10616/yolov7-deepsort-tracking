{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "We have 3 important files for this purpose and each and every dependency, class, import, function, variable etc is being imported from these modules\n",
    "\n",
    "1. `detection_helpers` which I made to wrap the original `YOLOv-7` code along with helper functions\n",
    "2. `tracking_helpers` has modular code which is used to wrap the `DeepSORT` repo and workings\n",
    "3. `bridge_wrapper` acts as a bridge to bind **ANY** detection model with `DeepSORT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detection_helpers import *\n",
    "from tracking_helpers import *\n",
    "from  bridge_wrapper import *\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection\n",
    "Detect objects using `Yolov-7`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      " Convert model to Traced-model... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\miniconda3\\envs\\tf_env\\lib\\site-packages\\torch\\_tensor.py:1104: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:475.)\n",
      "  return self._grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "detector = Detector(classes = [0]) # it'll detect ONLY [person,horses,sports ball]. class = None means detect all classes. List info at: \"data/coco.yaml\"\n",
    "detector.load_model('./weights/yolov7x.pt',) # pass the path to the trained weight file\n",
    "# ,17,32\n",
    "# # Pass in any image path or Numpy Image using 'BGR' format\n",
    "# result = detector.detect('./IO_data/input/images/horses.jpg', plot_bb = True) # plot_bb = False output the predictions as [x,y,w,h, confidence, class]\n",
    "\n",
    "\n",
    "# if len(result.shape) == 3:# If it is image, convert it to proper image. detector will give \"BGR\" image\n",
    "#     result = Image.fromarray(cv2.cvtColor(result,cv2.COLOR_BGR2RGB)) \n",
    "    \n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking\n",
    "\n",
    "Works as follows:\n",
    "1. Read each frame of video using `OpenCV`\n",
    "2. Get Bounding Box or Detections from the model per frame\n",
    "3. Crop those patches and pass on to `reID` model for re identification which is a part of `DeepSORT` method\n",
    "4. Get the above embeddings and then use `Kalman Filtering` and `Hungerian assignment` to assign the correct BB to the respective object\n",
    "5. Show, Save the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise  class that binds detector and tracker in one class\n",
    "tracker = YOLOv7_DeepSORT(reID_model_path=\"./deep_sort/model_weights/mars-small128.pb\", detector=detector)\n",
    "\n",
    "\n",
    "# output = None will not save the output video\n",
    "tracker.track_video(\"./IO_data/input/video/test_1.mp4\", output =\"./IO_data/output/test_1.avi\", show_live = True, skip_frames = 0, count_objects = True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show an MP4 video Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "\n",
    "mp4 = open(\"./IO_data/output/test_1.avi\",'rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"<video width=400 controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DeepSORT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd25a20b825ff2bb994666d7454c1bb113b6399af6d88567a1b4f41c2282da78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
